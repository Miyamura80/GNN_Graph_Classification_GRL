{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Miyamura80/GNN_Graph_Classification_GRL.git\n",
    "%cd GNN_Graph_Classification_GRL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_sc_graph(direc, file, processed_root):\n",
    "    path = osp.join(direc, file + \".json\")\n",
    "    presaved_path = osp.join(processed_root, file + \".pre\")\n",
    "    if not osp.exists(presaved_path):  # The file doesn't exist\n",
    "        print(\"making directory\")\n",
    "        with open(path, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "            data[0] = data[0][1:] + \"\"\n",
    "            data = [jline.strip()[:len(jline.strip())-1] for jline in data]\n",
    "\n",
    "            graphs = [json.loads(jline) for jline in data]\n",
    "            # Load Json into PyG `Data` type\n",
    "            pyg_graphs = [\n",
    "                map_sc_graph_to_pyg(graph, make_undirected=True, remove_dup=False)\n",
    "                for graph in graphs\n",
    "            ]\n",
    "\n",
    "            if not osp.exists(processed_root):\n",
    "                os.mkdir(processed_root)\n",
    "            with open(presaved_path, \"wb\") as g:  # Save for future reference\n",
    "                pickle.dump(pyg_graphs, g)\n",
    "                g.close()\n",
    "            f.close()\n",
    "            return pyg_graphs\n",
    "    else:  # Load the pre-existing pickle\n",
    "        print(\"load preexisting\")\n",
    "        with open(presaved_path, \"rb\") as g:\n",
    "            pyg_graphs = pickle.load(g)\n",
    "            g.close()\n",
    "        return pyg_graphs\n",
    "\n",
    "\n",
    "def map_sc_graph_to_pyg(json_file, make_undirected=True, remove_dup=False):\n",
    "    # Note: make_undirected makes duplicate edges, so we need to preserve edge types.\n",
    "    edge_index = np.array([[g[0], g[2]] for g in json_file[\"graph\"]]).T  # Edge Index\n",
    "    edge_attributes = np.array(\n",
    "        [g[1] - 1 for g in json_file[\"graph\"]]\n",
    "    )  # Edge type (-1 to put in [0, 3] range)\n",
    "\n",
    "    if (\n",
    "        make_undirected\n",
    "    ):  # This will invariably cost us edge types because we reduce duplicates\n",
    "        edge_index_reverse = edge_index[[1, 0], :]\n",
    "        # Concat and remove duplicates\n",
    "        if remove_dup:\n",
    "            edge_index = torch.LongTensor(\n",
    "                np.unique(\n",
    "                    np.concatenate([edge_index, edge_index_reverse], axis=1), axis=1\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            edge_index = torch.LongTensor(\n",
    "                np.concatenate([edge_index, edge_index_reverse], axis=1)\n",
    "            )\n",
    "            edge_attributes = torch.LongTensor(\n",
    "                np.concatenate([edge_attributes, np.copy(edge_attributes)], axis=0)\n",
    "            )\n",
    "    features = np.array(json_file[\"node_features\"])\n",
    "    features = np.concatenate((features[:, 0:11], features[:,22:36]), axis=1) \n",
    "    x = torch.FloatTensor(features)\n",
    "    y = torch.FloatTensor(np.array([[int(json_file[\"targets\"])]]).T)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attributes, y=y)\n",
    "\n",
    "\n",
    "def get_dataset(args, root_dir):\n",
    "    dataset_path = osp.join(root_dir, \"data\", args.dataset)\n",
    "    sc_proc_root = osp.join(dataset_path, f\"{args.dataset}_proc\")\n",
    "\n",
    "    train_graphs = read_sc_graph(\n",
    "        dataset_path, \"train\", sc_proc_root\n",
    "    )\n",
    "    valid_graphs = read_sc_graph(\n",
    "        dataset_path, \"valid\", sc_proc_root\n",
    "    )\n",
    "    num_feat = 25\n",
    "    num_pred = 1\n",
    "    return train_graphs, valid_graphs, num_feat, num_pred\n",
    "\n",
    "def fyi(x):\n",
    "    return 2*x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, BatchNorm1d\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_scatter import scatter_max, scatter_mean\n",
    "\n",
    "\n",
    "class NetGAT(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        num_classes,\n",
    "        emb_sizes=None,\n",
    "        drpt_prob=0.5,\n",
    "        scatter=\"max\",\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super(NetGAT, self).__init__()\n",
    "        if emb_sizes is None:  # Python default handling for mutable input\n",
    "            emb_sizes = [32, 64, 64]  # The 0th entry is the input feature size.\n",
    "        self.num_features = num_features\n",
    "        self.emb_sizes = emb_sizes\n",
    "        self.num_layers = len(self.emb_sizes) - 1\n",
    "        self.drpt_prob = drpt_prob\n",
    "        self.scatter = scatter\n",
    "        self.device = device\n",
    "\n",
    "        self.initial_mlp_modules = ModuleList(\n",
    "            [\n",
    "                Linear(num_features, emb_sizes[0]).to(device),\n",
    "                BatchNorm1d(emb_sizes[0]).to(device),\n",
    "                ReLU().to(device),\n",
    "                Linear(emb_sizes[0], emb_sizes[0]).to(device),\n",
    "                BatchNorm1d(emb_sizes[0]).to(device),\n",
    "                ReLU().to(device),\n",
    "            ]\n",
    "        )\n",
    "        self.initial_mlp = Sequential(*self.initial_mlp_modules).to(device)\n",
    "        self.initial_linear = Linear(emb_sizes[0], num_classes).to(device)\n",
    "\n",
    "        gat_layers = []\n",
    "        linears = []\n",
    "        for i in range(self.num_layers):\n",
    "            in_channel = emb_sizes[i]\n",
    "            out_channel = emb_sizes[i + 1]\n",
    "            gat_layer = GATConv(in_channels=in_channel, out_channels=out_channel).to(\n",
    "                device\n",
    "            )\n",
    "            gat_layers.append(gat_layer)\n",
    "            linears.append(Linear(emb_sizes[i + 1], num_classes).to(device))\n",
    "\n",
    "        self.gat_modules = ModuleList(gat_layers)\n",
    "        self.linear_modules = ModuleList(linears)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for (name, module) in self._modules.items():\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "        for module in self.gat_modules:\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "        for module in self.linear_modules:\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "        for module in self.initial_mlp_modules:\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "\n",
    "    def pooling(self, x_feat, batch):\n",
    "        if self.scatter == \"max\":\n",
    "            return scatter_max(x_feat, batch, dim=0)[0].to(self.device)\n",
    "        elif self.scatter == \"mean\":\n",
    "            return scatter_mean(x_feat, batch, dim=0).to(self.device)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_feat = data.x.to(self.device)\n",
    "        edge_index = data.edge_index.to(self.device)\n",
    "        edge_attributes = data.edge_attr.to(self.device)\n",
    "\n",
    "        x_feat = self.initial_mlp(x_feat)\n",
    "\n",
    "        out = F.dropout(\n",
    "            self.pooling(self.initial_linear(x_feat), data.batch), \n",
    "            p=self.drpt_prob\n",
    "        )\n",
    "\n",
    "        for gat_layer, linear_layer in zip(self.gat_modules, self.linear_modules):\n",
    "            edges = edge_index.T[edge_attributes == 1].T\n",
    "            x_feat = gat_layer(x_feat, edges).to(self.device)\n",
    "\n",
    "            out += F.dropout(\n",
    "                linear_layer(self.pooling(x_feat, data.batch)),\n",
    "                p=self.drpt_prob,\n",
    "                training=self.training,\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_max, scatter_mean\n",
    "from torch.nn import ModuleList, BatchNorm1d\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class NetGCN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        num_classes,\n",
    "        emb_sizes=None,\n",
    "        drpt_prob=0.5,\n",
    "        scatter=\"max\",\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super(NetGCN, self).__init__()\n",
    "        if emb_sizes is None:  # Python default handling for mutable input\n",
    "            emb_sizes = [32, 64, 64]  # The 0th entry is the input feature size.\n",
    "        self.num_features = num_features\n",
    "        self.emb_sizes = emb_sizes\n",
    "        self.num_layers = len(self.emb_sizes) - 1\n",
    "        self.drpt_prob = drpt_prob\n",
    "        self.scatter = scatter\n",
    "        self.device = device\n",
    "\n",
    "        self.initial_mlp_modules = ModuleList(\n",
    "            [\n",
    "                Linear(num_features, emb_sizes[0]).to(device),\n",
    "                BatchNorm1d(emb_sizes[0]).to(device),\n",
    "                ReLU().to(device),\n",
    "                Linear(emb_sizes[0], emb_sizes[0]).to(device),\n",
    "                BatchNorm1d(emb_sizes[0]).to(device),\n",
    "                ReLU().to(device),\n",
    "            ]\n",
    "        )\n",
    "        self.initial_mlp = Sequential(*self.initial_mlp_modules).to(device)\n",
    "        self.initial_linear = Linear(emb_sizes[0], num_classes).to(device)\n",
    "\n",
    "        gcn_layers = []\n",
    "        linears = []\n",
    "        for i in range(self.num_layers):\n",
    "            in_channel = emb_sizes[i]\n",
    "            out_channel = emb_sizes[i + 1]\n",
    "            gcn_layer = GCNConv(in_channels=in_channel, out_channels=out_channel).to(\n",
    "                device\n",
    "            )\n",
    "            gcn_layers.append(gcn_layer)\n",
    "            linears.append(Linear(emb_sizes[i + 1], num_classes).to(device))\n",
    "\n",
    "        self.gcn_modules = ModuleList(gcn_layers)\n",
    "        self.linear_modules = ModuleList(linears)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for (name, module) in self._modules.items():\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "        for module in self.gcn_modules:\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "        for module in self.linear_modules:\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "        for module in self.initial_mlp_modules:\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "\n",
    "    def pooling(self, x_feat, batch):\n",
    "        if self.scatter == \"max\":\n",
    "            return scatter_max(x_feat, batch, dim=0)[0].to(self.device)\n",
    "        elif self.scatter == \"mean\":\n",
    "            return scatter_mean(x_feat, batch, dim=0).to(self.device)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_feat = data.x.to(self.device)\n",
    "        edge_index = data.edge_index.to(self.device)\n",
    "        edge_attributes = data.edge_attr.to(self.device)\n",
    "\n",
    "        x_feat = self.initial_mlp(x_feat)\n",
    "\n",
    "        out = F.dropout(\n",
    "            self.pooling(self.initial_linear(x_feat), data.batch), \n",
    "            p=self.drpt_prob\n",
    "        )\n",
    "\n",
    "        for gcn_layer, linear_layer in zip(self.gcn_modules, self.linear_modules):\n",
    "            edges = edge_index.T[edge_attributes == 1].T\n",
    "            x_feat = gcn_layer(x_feat, edges).to(self.device)\n",
    "\n",
    "            out += F.dropout(\n",
    "                linear_layer(self.pooling(x_feat, data.batch)),\n",
    "                p=self.drpt_prob,\n",
    "                training=self.training,\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(args, device=\"cpu\", num_features=None, num_classes=None):\n",
    "\n",
    "    if args.model == \"GAT\":\n",
    "        emb_sizes = [args.emb_dim] * (args.num_layers + 1)\n",
    "        model = NetGAT(\n",
    "            num_features,\n",
    "            num_classes,\n",
    "            emb_sizes=emb_sizes,\n",
    "            device=device,\n",
    "            scatter=args.scatter,\n",
    "            drpt_prob=args.dropout,\n",
    "        )\n",
    "        return model\n",
    "    elif args.model == \"GCN\":\n",
    "        emb_sizes = [args.emb_dim] * (args.num_layers + 1)\n",
    "        model = NetGCN(\n",
    "            num_features,\n",
    "            num_classes,\n",
    "            emb_sizes=emb_sizes,\n",
    "            device=device,\n",
    "            scatter=args.scatter,\n",
    "            drpt_prob=args.dropout,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avail_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, loss_fun):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(avail_device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(model(data), data.y).to(avail_device)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_all / len(loader.dataset)\n",
    "\n",
    "\n",
    "def val(model, loader, loss_fun, y_idx=0):\n",
    "    model.eval()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(avail_device)\n",
    "        loss_all += loss_fun(model(data), data.y).item()\n",
    "\n",
    "    return loss_all / len(loader.dataset)\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    total_err = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(avail_device)\n",
    "        pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def run_sc_model_gc(\n",
    "    model,\n",
    "    dataset_tr,\n",
    "    dataset_val,\n",
    "    batch_size=32,\n",
    "    lr=0.0001,\n",
    "    epochs=300,\n",
    "    nb_reruns=5,\n",
    "):\n",
    "\n",
    "    plot_train_loss = []\n",
    "    plot_val_loss = []\n",
    "    plot_epoch = []\n",
    "\n",
    "    loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print(\"----------------- Predicting bug presence -----------------\")\n",
    "    all_val_loss = np.zeros(nb_reruns,)\n",
    "\n",
    "    for rerun in range(nb_reruns): \n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)  # Made static\n",
    "\n",
    "        val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False) # no shufflng for training\n",
    "        train_loader = DataLoader( # Shuffle for training\n",
    "            dataset_tr, batch_size=batch_size, shuffle=True\n",
    "        )  \n",
    "\n",
    "        print(\n",
    "            \"---------------- \"\n",
    "            + \": Re-run {} ----------------\".format(rerun)\n",
    "        )\n",
    "\n",
    "        best_val_loss = 100000\n",
    " \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # lr = scheduler.optimizer.param_groups[0]['lr']  # Same as GC\n",
    "            train_loss = train(\n",
    "                model, train_loader, optimizer, loss_fun\n",
    "            )\n",
    "            val_loss = val(model, val_loader, loss_fun)\n",
    "            # scheduler.step(val_mse_sum)\n",
    "            if best_val_loss >= val_loss:  # Improvement in validation loss\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "            # ======================================\n",
    "            # Plotting\n",
    "            # ======================================\n",
    "            \n",
    "            plot_train_loss.append(train_loss)\n",
    "            plot_epoch.append(epoch)\n",
    "            plot_val_loss.append(val_loss)\n",
    "\n",
    "\n",
    "            print(\n",
    "                \"Epoch: {:03d}, LR: {:7f}, Train Loss: {:.7f}, \"\n",
    "                \"Val Loss: {:.7f}\".format(\n",
    "                    epoch, lr, train_loss, val_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "        all_val_loss[rerun] = best_val_loss\n",
    "\n",
    "    # Calculate mean and standard deviation of validation results\n",
    "    avg_val_loss = all_val_loss.mean()\n",
    "    std_val_loss = np.std(all_val_loss)\n",
    "\n",
    "\n",
    "    torch.save(model, \"../model_eito.pt\")\n",
    "\n",
    "    plt.plot(plot_epoch, plot_train_loss, label = \"training loss\")\n",
    "    plt.plot(plot_epoch, plot_val_loss, label = \"validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"---------------- Final Result ----------------\")\n",
    "    print(\"Validation -- Mean: \" + str(avg_val_loss) + \", Std: \" + str(std_val_loss))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import os.path as osp\n",
    "from collections import DotMap\n",
    "\n",
    "args_dict = {\n",
    "    \"emb_dim\": 64,\n",
    "    \"dataset\": \"reentrancy\",\n",
    "    \"batch_size\": 32,\n",
    "    \"model\": \"GAT\",\n",
    "    \"lr\": 0.001,\n",
    "    \"max_distance\": 5,\n",
    "    \"num_layers\": 2,\n",
    "    \"emb_dim\": 64,\n",
    "    \"scatter\": \"max\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"eps\": 0.0,\n",
    "    \"epochs\": 300\n",
    "}\n",
    "args = DotMap(args_dict)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = osp.join(osp.dirname(osp.realpath(__file__)), \"..\")\n",
    "\n",
    "\n",
    "\n",
    "train_graphs, valid_graphs, num_feat, num_pred = get_dataset(args, root_dir)\n",
    "\n",
    "model = get_model(\n",
    "    args,\n",
    "    device,\n",
    "    num_features=num_feat,\n",
    "    num_classes=num_pred,\n",
    ")\n",
    "\n",
    "run_sc_model_gc(\n",
    "    model,\n",
    "    train_graphs,\n",
    "    valid_graphs,\n",
    "    lr=args.lr,\n",
    "    batch_size=args.batch_size,\n",
    "    epochs=args.epochs,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
